                 from  n    params  module                                  arguments
  0                -1  1      1760  models.common.Conv                      [3, 16, 6, 2, 2]
  1                -1  1      4672  models.common.Conv                      [16, 32, 3, 2]
  2                -1  1      4800  models.common.C3                        [32, 32, 1]
  3                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]
  4                -1  2     29184  models.common.C3                        [64, 64, 2]
  5                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]
  6                -1  3    156928  models.common.C3                        [128, 128, 3]
  7                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]
  8                -1  1    296448  models.common.C3                        [256, 256, 1]
  9                -1  1    164608  models.common.SPPF                      [256, 256, 5]
 10                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]
 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']
 12           [-1, 6]  1         0  models.common.Concat                    [1]
 13                -1  1     90880  models.common.C3                        [256, 128, 1, False]
 14                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]
 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']
 16           [-1, 4]  1         0  models.common.Concat                    [1]
 17                -1  1     22912  models.common.C3                        [128, 64, 1, False]
 18                -1  1     36992  models.common.Conv                      [64, 64, 3, 2]
 19          [-1, 14]  1         0  models.common.Concat                    [1]
 20                -1  1     74496  models.common.C3                        [128, 128, 1, False]
 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]
 22          [-1, 10]  1         0  models.common.Concat                    [1]
 23                -1  1    296448  models.common.C3                        [256, 256, 1, False]
 24      [17, 20, 23]  1      8118  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [64, 128, 256]]
YOLOv5n_w2y summary: 214 layers, 1765270 parameters, 1765270 gradients, 4.2 GFLOPs
Transferred 342/349 items from weights\yolov5n.pt
[34m[1mAMP: [39m[22mchecks passed
[34m[1moptimizer:[39m[22m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00046875), 60 bias
[34m[1mtrain: [39m[22mScanning D:\FEIJIEJIE\LUNA16_1\VOCdevkit\VOC\_train... 949 images, 0 backgrounds, 0 corrupt: 100%|██████████| 94
[34m[1mtrain: [39m[22mWARNING  Cache directory D:\FEIJIEJIE\LUNA16_1\VOCdevkit\VOC is not writeable: [WinError 183] : 'D:\\FEIJIEJIE\\LUNA16_1\\VOCdevkit\\VOC\\_train.cache.npy' -> 'D:\\FEIJIEJIE\\LUNA16_1\\VOCdevkit\\VOC\\_train.cache'
[34m[1mval: [39m[22mScanning D:\FEIJIEJIE\LUNA16_1\VOCdevkit\VOC\_val.cache... 119 images, 0 backgrounds, 0 corrupt: 100%|██████████|
[34m[1mAutoAnchor: [39m[22m3.32 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset
Plotting labels to yolo_test\fjj_v5n17\labels.jpg...
Image sizes 640 train, 640 val
Using 1 dataloader workers
Logging results to [1myolo_test\fjj_v5n17
Starting training for 120 epochs...
      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size
  0%|          | 0/80 [00:00<?, ?it/s].\train.py:323: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
      0/119      1.25G      0.101    0.03304          0         19        640:   1%|▏         | 1/80 [00:01<02:36,  1.9.\train.py:323: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients





