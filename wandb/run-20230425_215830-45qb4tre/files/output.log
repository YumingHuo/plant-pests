                 from  n    params  module                                  arguments
  0                -1  1      1760  models.common.Conv                      [3, 16, 6, 2, 2]
  1                -1  1      4672  models.common.Conv                      [16, 32, 3, 2]
  2                -1  1      4800  models.common.C3                        [32, 32, 1]
  3                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]
  4                -1  2     29184  models.common.C3                        [64, 64, 2]
  5                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]
  6                -1  3    156928  models.common.C3                        [128, 128, 3]
  7                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]
  8                -1  1    296448  models.common.C3                        [256, 256, 1]
  9                -1  1    164608  models.common.SPPF                      [256, 256, 5]
 10                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]
 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']
 12           [-1, 6]  1         0  models.common.Concat                    [1]
 13                -1  1     90880  models.common.C3                        [256, 128, 1, False]
 14                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]
 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']
 16           [-1, 4]  1         0  models.common.Concat                    [1]
 17                -1  1     22912  models.common.C3                        [128, 64, 1, False]
 18                -1  1     36992  models.common.Conv                      [64, 64, 3, 2]
 19          [-1, 14]  1         0  models.common.Concat                    [1]
 20                -1  1     74496  models.common.C3                        [128, 128, 1, False]
 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]
 22          [-1, 10]  1         0  models.common.Concat                    [1]
 23                -1  1    296448  models.common.C3                        [256, 256, 1, False]
 24      [17, 20, 23]  1      8118  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [64, 128, 256]]
YOLOv5n_w2y summary: 214 layers, 1765270 parameters, 1765270 gradients, 4.2 GFLOPs
Transferred 342/349 items from weights\yolov5n.pt
[34m[1mAMP: [39m[22mchecks passed
[34m[1moptimizer:[39m[22m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00046875), 60 bias
[34m[1mtrain: [39m[22mScanning D:\FEIJIEJIE\LUNA16_1\VOCdevkit\VOC\_train... 949 images, 0 backgrounds, 0 corrupt: 100%|██████████| 94
[34m[1mtrain: [39m[22mWARNING  Cache directory D:\FEIJIEJIE\LUNA16_1\VOCdevkit\VOC is not writeable: [WinError 183] : 'D:\\FEIJIEJIE\\LUNA16_1\\VOCdevkit\\VOC\\_train.cache.npy' -> 'D:\\FEIJIEJIE\\LUNA16_1\\VOCdevkit\\VOC\\_train.cache'
[34m[1mval: [39m[22mScanning D:\FEIJIEJIE\LUNA16_1\VOCdevkit\VOC\_val.cache... 119 images, 0 backgrounds, 0 corrupt: 100%|██████████| 119/119 [00:00<?, ?it/s]
[34m[1mAutoAnchor: [39m[22m3.32 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset
Plotting labels to yolo_test\base_n5\labels.jpg...
Image sizes 640 train, 640 val
Using 1 dataloader workers
Logging results to [1myolo_test\base_n5
Starting training for 120 epochs...
      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size
  0%|          | 0/48 [00:00<?, ?it/s].\train.py:323: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
      0/119      1.88G     0.1489    0.03245          0         29        640:   2%|▏         | 1/48 [00:01<01:33,  1.99s/it].\train.py:323: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients
      0/119      2.04G     0.1256    0.03225          0         31        640:  17%|█▋        | 8/48 [00:03<00:11,  3.38it/s].\train.py:323: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients




      0/119      2.04G      0.126    0.02257          0         38        640:  73%|███████▎  | 35/48 [00:16<00:06,  2.16it/s]
Traceback (most recent call last):
  File ".\train.py", line 640, in <module>
    main(opt)
  File ".\train.py", line 529, in main
    train(opt.hyp, opt, device, callbacks)
  File ".\train.py", line 284, in train
    for i, (imgs, targets, paths, _) in pbar:  # batch -------------------------------------------------------------
  File "D:\Miniconda3\envs\torch_gpu\lib\site-packages\tqdm\std.py", line 1178, in __iter__
    for obj in iterable:
  File "D:\FEIJIEJIE\yolov5\utils\dataloaders.py", line 172, in __iter__
    yield next(self.iterator)
  File "D:\Miniconda3\envs\torch_gpu\lib\site-packages\torch\utils\data\dataloader.py", line 521, in __next__
    data = self._next_data()
  File "D:\Miniconda3\envs\torch_gpu\lib\site-packages\torch\utils\data\dataloader.py", line 1186, in _next_data
    idx, data = self._get_data()
  File "D:\Miniconda3\envs\torch_gpu\lib\site-packages\torch\utils\data\dataloader.py", line 1142, in _get_data
    success, data = self._try_get_data()
  File "D:\Miniconda3\envs\torch_gpu\lib\site-packages\torch\utils\data\dataloader.py", line 990, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "D:\Miniconda3\envs\torch_gpu\lib\queue.py", line 179, in get
    self.not_empty.wait(remaining)
  File "D:\Miniconda3\envs\torch_gpu\lib\threading.py", line 306, in wait
    gotit = waiter.acquire(True, timeout)
KeyboardInterrupt